#include <ATen/ATen.h>
#include <c10/xpu/XPUStream.h>
#include <torch/all.h>

#include <cute/tensor.hpp>

#include "Utils.h"
#include "cutlass/epilogue/collective/collective_builder.hpp"
#include "cutlass/epilogue/collective/default_epilogue.hpp"
#include "cutlass/epilogue/fusion/xe_callbacks.hpp"
#include "cutlass/gemm/collective/collective_mma.hpp"
#include "cutlass/gemm/device/gemm_universal.h"
#include "cutlass/gemm/group_array_problem_shape.hpp"
#include "cutlass/util/device_memory.h"
#include "kernels/moe/dispatch_policy.hpp"
#include "kernels/moe/xe_array_epilogue.hpp"
#include "kernels/moe/xe_array_mma.hpp"
#include "kernels/moe/xe_moe_gemm.hpp"

using namespace cute;

template <typename scalar_t>
struct MoERunner {
  using ProblemShape = cutlass::gemm::GroupProblemShape<Shape<int, int, int>>;  // <M,N,K> per group
  template <typename Gemm>
  typename Gemm::Arguments args_from_options(
      const cutlass::KernelHardwareInfo& hw_info,
      const typename Gemm::ElementA* A_ptr,
      const typename Gemm::ElementB* B_ptr,
      typename Gemm::CollectiveEpilogue::ElementOutput* D_ptr,
      const int gemm_N,
      const int gemm_K,
      const int* num_rows_per_expert_device,
      const int num_experts) {
    typename Gemm::Arguments arguments;
    decltype(arguments.fusion_args) fusion_args;

    fusion_args.alpha = 1;
    fusion_args.beta = 0;
    fusion_args.alpha_ptr = nullptr;
    fusion_args.beta_ptr = nullptr;
    fusion_args.alpha_ptr_array = nullptr;
    fusion_args.beta_ptr_array = nullptr;
    // One alpha and beta per each group
    fusion_args.dAlpha = {cute::_0{}, cute::_0{}, 1};
    fusion_args.dBeta = {cute::_0{}, cute::_0{}, 1};

    using RasterOrderOptions =
        typename cutlass::gemm::kernel::detail::PersistentTileSchedulerXeGroup<ProblemShape>::RasterOrderOptions;

    arguments = typename Gemm::Arguments{
        cutlass::gemm::GemmUniversalMode::kGrouped,
        static_cast<const typename Gemm::ElementA**>((void*)A_ptr),
        static_cast<const typename Gemm::ElementB**>((void*)B_ptr),
        nullptr,  // static_cast<const ElementC**>((void*)D_ptr),
        static_cast<typename Gemm::CollectiveEpilogue::ElementOutput**>((void*)D_ptr),
        fusion_args,
        hw_info,
        {1, RasterOrderOptions::AlongN},
        num_rows_per_expert_device,
        num_experts,
        gemm_N,
        gemm_K};

    return arguments;
  }

  int init(
      int device_id,
      const void* activations,
      const void* weights,
      void* outputs,
      const int gemm_n,
      const int gemm_k,
      const int* num_rows_per_expert_device,
      const int num_experts) {  // Change device_id to another value if you are running on a machine with
    // multiple GPUs and wish to use a GPU other than that with device ID 0.
    hw_info.sm_count = cutlass::KernelHardwareInfo::query_device_multiprocessor_count(hw_info.device_id);
    gemm_args = args_from_options<Gemm>(
        hw_info,
        reinterpret_cast<const scalar_t*>(activations),
        reinterpret_cast<const scalar_t*>(weights),
        reinterpret_cast<scalar_t*>(outputs),
        gemm_n,
        gemm_k,
        num_rows_per_expert_device,
        num_experts);
    TORCH_CHECK(gemm_op.can_implement(gemm_args) == cutlass::Status::kSuccess, "GEMM configuration not supported.");
    return Gemm::get_workspace_size(gemm_args);
  }

  void run(sycl::queue queue, void* workspace) {
    TORCH_CHECK(gemm_op.initialize(gemm_args, workspace) == cutlass::Status::kSuccess, "Failed to initialize GEMM.");

    // Run the GEMM
    TORCH_CHECK(gemm_op.run(&queue) == cutlass::Status::kSuccess, "Failed to run GEMM.");
  }

 public:
  using LayoutA = cutlass::layout::RowMajor;
  using LayoutB = cutlass::layout::ColumnMajor;
  using LayoutC = cutlass::layout::RowMajor;
  using LayoutD = cutlass::layout::RowMajor;

  using GmemTiledCopyA = XE_2D_U16x8x32_LD_N;
  using GmemTiledCopyB = XE_2D_U16x16x16_LD_T;

  // Workgroup-level tile
  using TileShape = Shape<_256, _256, _32>;

  using TiledMma =  // M=8,N=16,K=16, D=f32,A=bf16,B=bf16,C=f32
      typename TiledMMAHelper<
          MMA_Atom<XE_8x16x16_F32BF16BF16F32_TT>,
          Layout<TileShape>,
          Layout<Shape<_8, _4, _1>, Stride<_4, _1, _0>>>::TiledMMA;

  static constexpr int PipelineStages = 2;
  // Dispatch to grouped gemm algorithm
  using GEMMDispatchPolicy = cutlass::gemm::MainloopIntelXeXMX16MoE<PipelineStages, cutlass::gemm::KernelXeMoEGEMM>;
  using EpilogueDispatchPolicy = cutlass::epilogue::IntelXeXMX16Group;

  // ScaledAcc needs to be supported in xe_builder.inl and xe_callbacks.cpp
  // This is a workaround
  using EpilogueOp = cutlass::epilogue::fusion::
      LinearCombination<float_t, float_t, float_t, float_t, cutlass::FloatRoundStyle::round_to_nearest>;
  using CopyOpG2R = XE_2D_U32x8x16_LD_N;
  using CopyOpR2G = XE_2D_U16x8x16_ST_N;

  using StrideC = cutlass::detail::TagToStrideC_t<LayoutC*>;
  using FusionCallbacks = typename cutlass::epilogue::collective::detail::FusionOpInfo<
      EpilogueOp>::template FusionCallbacks<cutlass::epilogue::IntelXeXMX16Group, TileShape, TileShape, CopyOpG2R>;
  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveEpilogue<
      cutlass::epilogue::IntelXeXMX16MoE,
      TileShape,
      float,
      StrideC,
      scalar_t,
      StrideC,
      FusionCallbacks,
      CopyOpG2R,
      void,
      void,
      CopyOpR2G,
      void,
      void>;

  // Mainloop
  using CollectiveMainloop = cutlass::gemm::collective::CollectiveMma<
      GEMMDispatchPolicy,
      TileShape,
      scalar_t,
      cutlass::gemm::TagToStrideA_t<LayoutA*>,
      scalar_t,
      cutlass::gemm::TagToStrideB_t<LayoutB*>,
      TiledMma,
      GmemTiledCopyA,
      void,
      void,
      cute::identity,  // A
      GmemTiledCopyB,
      void,
      void,
      cute::identity  // B
      >;

  using GemmKernel = cutlass::gemm::kernel::
      GemmMoEUniversal<ProblemShape, CollectiveMainloop, CollectiveEpilogue, cutlass::gemm::GroupScheduler>;

  using Gemm = cutlass::gemm::device::GemmMoEUniversalAdapter<GemmKernel>;

  Gemm gemm_op;
  typename Gemm::Arguments gemm_args;
  // The KernelHardwareInfo struct holds the number of EUs on the GPU with a
  // given device ID. This information is used by the underlying kernel.
  cutlass::KernelHardwareInfo hw_info;
};

void moe_grouped_mm_nt(
    torch::Tensor& output,
    const torch::Tensor& activations,
    const torch::Tensor& weights,
    const torch::Tensor& total_rows_for_experts,
    const int64_t n_experts) {
  int total_m = activations.sizes()[0];
  int gemm_k = activations.sizes()[1];
  auto weights_shape = weights.sizes().vec();
  int gemm_n = weights.sizes()[1];

  TORCH_CHECK(weights_shape.size() == 3, "weights must be 3D");
  TORCH_CHECK(weights_shape[0] == n_experts, "weights must have n_experts as the first dimension");
  TORCH_CHECK(weights_shape[1] == gemm_n, "weights must be gemm_n * gemm_k");
  TORCH_CHECK(
      weights_shape[0] == total_rows_for_experts.size(0),
      "rows_for_experts must have the same size as the first dimension of weights");
  TORCH_CHECK(output.sizes()[0] == total_m, "output must have the same number of rows as activations");
  TORCH_CHECK(output.sizes()[1] == gemm_n, "output must have the same number of columns as activations");
  TORCH_CHECK(n_experts % 8 == 0, "n_experts must be a multiple of 8 for the current implementation");
  TORCH_CHECK(
      activations.scalar_type() == weights.scalar_type(), "activations and weights must have the same data type");
  TORCH_CHECK(
      activations.scalar_type() == at::ScalarType::BFloat16,
      "Only bfloat16 are supported in moe_grouped_mm_nt currently");

  auto stream = at::xpu::getCurrentXPUStream();
  auto queue = stream.queue();

  using Kernel = MoERunner<cutlass::bfloat16_t>;
  Kernel kernel;
  auto workspace_size = kernel.init(
      activations.device().index(),
      activations.data_ptr(),
      weights.data_ptr(),
      output.data_ptr(),
      gemm_n,
      gemm_k,
      total_rows_for_experts.data_ptr<int>(),
      n_experts);
  auto const workspace_options = torch::TensorOptions().dtype(torch::kUInt8).device(activations.device());
  auto workspace = torch::empty(workspace_size, workspace_options);
  kernel.run(queue, workspace.data_ptr());
}
